Architectures
    Feedforward
        Residual Networks
        Highway Networks
        Dense Networks
        Ladder Networks (Semi-supervised Learning)
        Stochastic Depth Networks
    Recurrent
        Gated Recurrent Unit
        Long Short Term Memory
        Recurrent Highway Units
        Bidirectionality
        Attention Mechanisms
        Sequence to Sequence Models
        Dynamic Memory Networks
    Recursive
        Recursive Neural Tensor Network
        Recursive LSTMs
    Convolutional
        Max Pooling
        Fractional Maxpool
    Restricted Boltzmann Machines
        Deep Belief Nets
        Autoencoders
            Denoising
            Variational
    Word2vec, Doc2Vec
    Deep Reinforcement Learning
    Generative Adversarial Networks
        Wasserstein GANs

Activation Functions
    sigmoid
    tanh
    Rectified Linear Units
        Leaky ReLUs
    Exponential Linear Units
    maxout
    softmax

Cost Functions
    squared error
    cross entropy
    max margin
   
Gradient Descent
    batch, minibatch, online
    momentum
    rprop and rmsprop
    adagrad and adadelta
    adam
   
    Newton's Method
    Limited Memory Broyden–Fletcher–Goldfarb–Shanno Algorithm
    Hessian Free Optimization and Conjugate Gradients
   
Preventing Overfitting
    Add More Data
        Bootstrapping, Patch Selection (for images), and Synthetic Data
    Use a Simpler Model
    Early Stopping based on Validation Set Accuracy
    L1/L2 Regularization
    Add Gaussian Noise to Intpus or Weights
    Average Different Models (e.g. Boosting/Bagging)
    Dropout Regularization
   
Optimization Tricks
    Input Feature Mean/Std Normalization
    Xavier Weight Initialization Based on Fan-In/Fan-Out
    Orthogonal Weight Initialization or Identity Matrix Initialization
    Batch Normalization and Layer Normalization
    Word Vector Initialization for NLP Applications
    Gradient Clipping for Recurrent NNs
    Scheduled Sampling for Sequence Prediction Tasks
    Generatively pre-train deep nets using autoencoding layers
    
Distributed Computing and GPU Optimization
    Model Parallelism
    Data Parallelism
    Downpour SGD
    Elastic Averaging SGD
  
Other Applications
    Feature Reduction and Selection
    Predicting Input Similarity By Comparing Learned Feature Layers

NN Libraries and Tools
    theano
        lasagne
        keras
    tensorflow
    microsoft cntk
    caffe
    pytorch
    mxnet